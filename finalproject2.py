# -*- coding: utf-8 -*-
"""FinalProject2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14e1SVJ0WJVu2BFFJnyMG0wZBRKORyKff
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
!pip install tensorflow torch torchvision scikit-learn numpy opencv-python

# Download the CROHME dataset
!wget -O CROHME23.zip "https://zenodo.org/record/8428035/files/CROHME23.zip?download=1"

# Unzip the dataset
!unzip CROHME23.zip -d dataset
!ls dataset
import os
import cv2
import numpy as np
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.utils import to_categorical

# Valid image extensions
valid_extensions = ('.png', '.jpg', '.jpeg')
# Set dataset path
dataset_path = 'dataset/TC11_CROHME23'  # Adjust based on the exact unzipped path

def load_data(dataset_path):
    images, labels = [], []

    # Go through each split (train, val, test)
    for split_dir in os.listdir(dataset_path):
        split_path = os.path.join(dataset_path, split_dir)

        if os.path.isdir(split_path):
            # Go through each label directory inside the split
            for label_dir in os.listdir(split_path):
                label_path = os.path.join(split_path, label_dir)

                if os.path.isdir(label_path):
                    # Process images in the label directory
                    print(f"Processing label directory: {label_path}")

                    for image_file in os.listdir(label_path):
                        if not image_file.lower().endswith(valid_extensions):
                            print(f"Skipping non-image file: {image_file}")
                            continue

                        image_path = os.path.join(label_path, image_file)
                        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale

                        if image is not None:  # Check if image was loaded successfully
                            image = cv2.resize(image, (64, 64))  # Resize all images to 64x64
                            images.append(image)
                            labels.append(label_dir)  # Label is the folder name
                        else:
                            print(f"Warning: Could not load image {image_path}")

    # Convert lists to arrays
    images = np.array(images, dtype="float32") / 255.0  # Normalize pixel values
    labels = np.array(labels)

    return images, labels

# Load the dataset
images, labels = load_data(dataset_path)
print(f"Loaded {len(images)} images with {len(set(labels))} unique labels.")

def parse_lg_file(file_path):
    # Example placeholder: read .lg file as text or structured data
    with open(file_path, 'r') as f:
        content = f.read()
    # Process content as needed. This example converts text to ASCII codes.
    return [ord(char) for char in content[:100]]  # Adjust as needed

def load_data(dataset_path):
    images, labels = [], []

    # Process files in each label directory
    for split_dir in os.listdir(dataset_path):
        split_path = os.path.join(dataset_path, split_dir)
        if os.path.isdir(split_path):
            for label_dir in os.listdir(split_path):
                label_path = os.path.join(split_path, label_dir)
                if os.path.isdir(label_path):
                    for file_name in os.listdir(label_path):
                        file_path = os.path.join(label_path, file_name)
                        if file_name.lower().endswith(valid_extensions):
                            # Process image file
                            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
                            if image is not None:
                                image = cv2.resize(image, (64, 64))
                                images.append(image)
                                labels.append(label_dir)
                        elif file_name.lower().endswith('.lg'):
                            # Process .lg file
                            parsed_data = parse_lg_file(file_path)
                            images.append(parsed_data)  # Replace 'images' with a suitable list if needed
                            labels.append(label_dir)

    images = np.array(images, dtype="float32") / 255.0 if images else np.array([])
    labels = np.array(labels) if labels else np.array([])
    return images, labels

import os

def print_directory_structure(path, level=0):
    indent = "    " * level
    for item in os.listdir(path):
        item_path = os.path.join(path, item)
        if os.path.isdir(item_path):
            print(f"{indent}Directory: {item}")
            print_directory_structure(item_path, level + 1)
        else:
            print(f"{indent}File: {item}")

# Print the full directory structure of the dataset
print("Full Directory Structure:")
print_directory_structure(dataset_path)

def parse_lg_file(file_path):
    # Read .lg file as text
    with open(file_path, 'r') as file:
        content = file.read()

    # Example processing: Convert first 100 characters to ASCII values
    processed_content = np.array([ord(char) for char in content[:100]])  # Adjust processing as needed
    return processed_content

# Load the dataset
data, labels = load_data(dataset_path)
print(f"Loaded {len(data)} samples with {len(labels)} labels.")
print("Sample data:", data[:3])       # Print the first few samples for inspection
print("Sample labels:", labels[:3])   # Print the first few labels

def load_data(dataset_path):
    images, labels = [], []

    # Traverse dataset structure
    for split_dir in os.listdir(dataset_path):
        split_path = os.path.join(dataset_path, split_dir)

        if os.path.isdir(split_path):
            # Process each label directory inside the split
            for label_dir in os.listdir(split_path):
                label_path = os.path.join(split_path, label_dir)

                if os.path.isdir(label_path):
                    print(f"Processing label directory: {label_path}")

                    # Process each file in the label directory
                    for file_name in os.listdir(label_path):
                        file_path = os.path.join(label_path, file_name)

                        # Print file details for debugging
                        print(f"Found file: {file_path} - Size: {os.path.getsize(file_path)} bytes")

                        # Check if the file is an image or .lg file
                        if file_name.lower().endswith(valid_extensions) or '.' not in file_name:
                            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
                            if image is not None:
                                image = cv2.resize(image, (64, 64))
                                images.append(image)
                                labels.append(label_dir)
                            else:
                                print(f"Warning: Could not load image {file_path}")
                        elif file_name.lower().endswith('.lg'):
                            parsed_data = parse_lg_file(file_path)
                            images.append(parsed_data)
                            labels.append(label_dir)

    images = np.array(images, dtype="float32") / 255.0 if images else np.array([])
    labels = np.array(labels) if labels else np.array([])
    return images, labels

# Run the function with detailed debug output
images, labels = load_data(dataset_path)
print(f"Loaded {len(images)} samples with {len(set(labels))} unique labels.")

def load_data(dataset_path):
    images, labels = [], []

    # Recursive function to search for files
    def find_files_in_directory(path, label):
        for item in os.listdir(path):
            item_path = os.path.join(path, item)
            if os.path.isdir(item_path):
                # Recursively search inside subdirectories
                find_files_in_directory(item_path, label)
            elif item_path.lower().endswith(valid_extensions) or '.' not in item_path:
                # Process only image files or files without extensions
                image = cv2.imread(item_path, cv2.IMREAD_GRAYSCALE)
                if image is not None:
                    image = cv2.resize(image, (64, 64))
                    images.append(image)
                    labels.append(label)
                    print(f"Added image with shape: {image.shape}")  # Debugging line to confirm shape
                else:
                    print(f"Warning: Could not load image {item_path}")

    # Traverse dataset structure
    for split_dir in os.listdir(dataset_path):
        split_path = os.path.join(dataset_path, split_dir)
        if os.path.isdir(split_path):
            for label_dir in os.listdir(split_path):
                label_path = os.path.join(split_path, label_dir)
                if os.path.isdir(label_path):
                    print(f"Processing label directory: {label_path}")
                    # Call recursive file search within each label directory
                    find_files_in_directory(label_path, label_dir)

    # Convert lists to arrays
    images = np.array(images, dtype="float32") / 255.0 if images else np.array([])
    labels = np.array(labels) if labels else np.array([])
    return images, labels

# Load and inspect data
images, labels = load_data(dataset_path)
print(f"Loaded {len(images)} samples with {len(set(labels))} unique labels.")

from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.utils import to_categorical

# One-hot encode the labels
label_binarizer = LabelBinarizer()
labels = label_binarizer.fit_transform(labels)
labels = to_categorical(labels)  # Convert to categorical format

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Reshape X_train and X_test to add a channel dimension for CNN input
X_train = X_train.reshape(-1, 64, 64, 1)  # (samples, height, width, channels)
X_test = X_test.reshape(-1, 64, 64, 1)

from tensorflow.keras import layers, models, Input

def create_cnn_model(input_shape, num_classes):
    model = models.Sequential([
        Input(shape=input_shape),  # Explicitly add an Input layer
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

# Instantiate and compile the model
model = create_cnn_model((64, 64, 1), len(label_binarizer.classes_))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Check the structure of labels before transformation
print("Labels type:", type(labels))
print("Labels shape:", labels.shape)
print("Sample labels data:", labels[:5])  # Display first 5 items to inspect structure

import numpy as np

# Flatten labels if it has extra dimensions
labels = np.squeeze(labels)  # This will remove any dimensions of size 1

print("Labels shape after squeezing:", labels.shape)

from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.utils import to_categorical

# One-hot encode only if necessary
if labels.ndim == 1 or (labels.ndim == 2 and labels.shape[1] == 1):  # Not yet one-hot encoded
    label_binarizer = LabelBinarizer()
    labels = label_binarizer.fit_transform(labels)
    labels = to_categorical(labels)  # Convert to categorical format if not already

# Verify the final shape
print(f"Final labels shape for training: {labels.shape}")

# Flatten the labels array to ensure it's two-dimensional
labels = labels.reshape(labels.shape[0], -1)  # Reshape to (27279, 3)

# Verify the final shape
print(f"Final labels shape for training: {labels.shape}")

# Squeeze the labels to remove any unnecessary dimensions
y_train = np.squeeze(y_train)
y_test = np.squeeze(y_test)

# Verify the shape after squeezing
print("y_train shape after squeezing:", y_train.shape)
print("y_test shape after squeezing:", y_test.shape)

# Flatten the labels to remove the extra dimension
y_train = y_train.reshape(-1, y_train.shape[-1])
y_test = y_test.reshape(-1, y_test.shape[-1])

# Verify the shapes after reshaping
print("y_train shape after reshaping:", y_train.shape)
print("y_test shape after reshaping:", y_test.shape)

X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

import numpy as np

# Verify unique classes in y_train and y_test
unique_classes_train = np.unique(np.argmax(y_train, axis=1))
unique_classes_test = np.unique(np.argmax(y_test, axis=1))
print("Unique classes in y_train:", unique_classes_train)
print("Unique classes in y_test:", unique_classes_test)
print("Expected number of classes:", len(unique_classes_train))

from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer

label_binarizer = LabelBinarizer()
label_binarizer.fit([0, 1, 2])  # Ensure only 3 classes

# One-hot encode y_train and y_test to match 3 classes
y_train = to_categorical(label_binarizer.transform(np.argmax(y_train, axis=1)))
y_test = to_categorical(label_binarizer.transform(np.argmax(y_test, axis=1)))

print("y_train shape after encoding:", y_train.shape)
print("y_test shape after encoding:", y_test.shape)

# Ensure y_train and y_test are two-dimensional
if y_train.ndim > 2:
    y_train = y_train.reshape(y_train.shape[0], -1)
if y_test.ndim > 2:
    y_test = y_test.reshape(y_test.shape[0], -1)

# Print to confirm
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Input

num_classes = labels.shape[1]  # Adjust based on the actual number of unique classes in the dataset
print(f"Number of classes: {num_classes}")

# Define the model with an Input layer
model = Sequential([
    Input(shape=(64, 64, 1)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')  # Dynamically set to match the number of classes
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f"Test accuracy: {test_accuracy}")
print(f"Test loss: {test_loss}")
print("Model defined and compiled successfully.")

from tensorflow.keras.regularizers import l2

model = Sequential([
    Input(shape=(64, 64, 1)),
    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),  # Dropout layer

    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.3),  # Dropout layer

    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.4),  # Dropout layer

    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    layers.Dropout(0.5),  # Dropout layer
    layers.Dense(num_classes, activation='softmax')
])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import LearningRateScheduler

# Initialize the data generator
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

# Fit the data generator to the training data
datagen.fit(X_train)  # Fit must happen before model.fit()

# Define the model
model = Sequential([
    Input(shape=(64, 64, 1)),  # Input layer
    Conv2D(32, (3, 3), activation='relu'),  # First Conv2D layer
    MaxPooling2D((2, 2)),  # First MaxPooling2D layer
    Conv2D(64, (3, 3), activation='relu'),  # Second Conv2D layer
    MaxPooling2D((2, 2)),  # Second MaxPooling2D layer
    Flatten(),  # Flatten for dense layers
    Dense(128, activation='relu'),  # Fully connected layer
    Dense(num_classes, activation='softmax')  # Output layer
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define the learning rate scheduler
def scheduler(epoch, lr):
    return lr * 0.1 if epoch > 10 else lr

callback = LearningRateScheduler(scheduler)

# Train the model
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),  # Training with data augmentation
    epochs=20,
    validation_data=(X_test, y_test),
    callbacks=[callback]
)

model = Sequential([
    Input(shape=(64, 64, 1)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(256, (3, 3), activation='relu'),  # Additional convolutional layer
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),  # Increase the size of the dense layer
    layers.Dense(num_classes, activation='softmax')
])

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense

# Define the model
model = Sequential([
    Input(shape=(64, 64, 1)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(num_classes, activation='softmax')
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define EarlyStopping to stop training if no improvement in validation loss
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

# Define ModelCheckpoint to save the best model during training
model_checkpoint = ModelCheckpoint(
    'best_model.keras',  # Use .keras format as required by Keras
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

# Train the model
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=50,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, model_checkpoint]
)

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

# Normalize the data
X_train = X_train / 255.0  # Scale pixel values to [0, 1]
X_test = X_test / 255.0

# Ensure input data shape is correct
if X_train.ndim == 3:  # Add channel dimension for grayscale images
    X_train = np.expand_dims(X_train, axis=-1)
if X_test.ndim == 3:
    X_test = np.expand_dims(X_test, axis=-1)

print("X_train shape:", X_train.shape)  # Expect (num_samples, 64, 64, 1)
print("X_test shape:", X_test.shape)    # Expect (num_samples, 64, 64, 1)

# Flatten labels to remove extra dimensions if necessary
if y_train.ndim > 2:
    y_train = y_train.reshape(y_train.shape[0], -1)  # Reshape to (num_samples, num_classes)
if y_test.ndim > 2:
    y_test = y_test.reshape(y_test.shape[0], -1)    # Reshape to (num_samples, num_classes)

# Reapply one-hot encoding
num_classes = len(np.unique(np.argmax(y_train, axis=-1)))  # Dynamically determine the number of classes
y_train = to_categorical(np.argmax(y_train, axis=-1), num_classes=num_classes)
y_test = to_categorical(np.argmax(y_test, axis=-1), num_classes=num_classes)

# Verify final label shapes
print("y_train shape:", y_train.shape)  # Expect (num_samples, num_classes)
print("y_test shape:", y_test.shape)

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)
datagen.fit(X_train)  # Fit generator to training data

# Define the model
model = Sequential([
    Input(shape=(64, 64, 1)),
    Conv2D(32, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.2),  # Regularization

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.3),  # Regularization

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.4),  # Regularization

    Flatten(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),  # Regularization
    Dense(num_classes, activation='softmax')  # Output layer with num_classes
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1,
    min_lr=1e-6
)

# Train the model
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=50,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, model_checkpoint, reduce_lr]
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f"Test accuracy: {test_accuracy:.2f}, Test loss: {test_loss:.2f}")

# Plot learning curves
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam

# Normalize the data
X_train = X_train / 255.0  # Scale pixel values to [0, 1]
X_test = X_test / 255.0

# Ensure input data shape is correct
if X_train.ndim == 3:  # Add channel dimension for grayscale images
    X_train = np.expand_dims(X_train, axis=-1)
if X_test.ndim == 3:
    X_test = np.expand_dims(X_test, axis=-1)

print("X_train shape:", X_train.shape)  # Expect (num_samples, 64, 64, 1)
print("X_test shape:", X_test.shape)    # Expect (num_samples, 64, 64, 1)

# Reapply one-hot encoding
num_classes = len(np.unique(np.argmax(y_train, axis=-1)))  # Dynamically determine the number of classes
y_train = to_categorical(np.argmax(y_train, axis=-1), num_classes=num_classes)
y_test = to_categorical(np.argmax(y_test, axis=-1), num_classes=num_classes)

print("y_train shape:", y_train.shape)  # Expect (num_samples, num_classes)
print("y_test shape:", y_test.shape)

# Enhanced Data augmentation
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    shear_range=0.2
)
datagen.fit(X_train)

# Define a regularized and simplified model
model = Sequential([
    Input(shape=(64, 64, 1)),
    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.3),  # Increased dropout

    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.4),  # Increased dropout

    Flatten(),
    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.5),  # Strong dropout
    Dense(num_classes, activation='softmax')  # Output layer
])

# Compile the model with a smaller learning rate
model.compile(
    optimizer=Adam(learning_rate=5e-4),  # Start with a slightly reduced learning rate
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Learning Rate Scheduler for warmup
def warmup(epoch):
    if epoch < 5:
        return 5e-5  # Start small during initial epochs
    return 5e-4  # Default learning rate after warmup

lr_scheduler = LearningRateScheduler(warmup)

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=8,
    restore_best_weights=True,
    verbose=1
)

model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1,
    min_lr=1e-6
)

# Train the model
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=100,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, model_checkpoint, reduce_lr, lr_scheduler]
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f"Test accuracy: {test_accuracy:.2f}, Test loss: {test_loss:.2f}")

# Plot learning curves
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam

# Normalize the data
X_train = X_train / 255.0  # Scale pixel values to [0, 1]
X_test = X_test / 255.0

# Ensure input data shape is correct
if X_train.ndim == 3:  # Add channel dimension for grayscale images
    X_train = np.expand_dims(X_train, axis=-1)
if X_test.ndim == 3:
    X_test = np.expand_dims(X_test, axis=-1)

print("X_train shape:", X_train.shape)  # Expect (num_samples, 64, 64, 1)
print("X_test shape:", X_test.shape)    # Expect (num_samples, 64, 64, 1)

# Reapply one-hot encoding
num_classes = len(np.unique(np.argmax(y_train, axis=-1)))  # Dynamically determine the number of classes
y_train = to_categorical(np.argmax(y_train, axis=-1), num_classes=num_classes)
y_test = to_categorical(np.argmax(y_test, axis=-1), num_classes=num_classes)

print("y_train shape:", y_train.shape)  # Expect (num_samples, num_classes)
print("y_test shape:", y_test.shape)

# Enhanced Data augmentation
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    shear_range=0.2
)
datagen.fit(X_train)

# Define a regularized and simplified model
model = Sequential([
    Input(shape=(64, 64, 1)),
    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.3),  # Increased dropout

    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.4),  # Increased dropout

    Flatten(),
    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.5),  # Strong dropout
    Dense(num_classes, activation='softmax')  # Output layer
])

# Compile the model with a smaller learning rate
model.compile(
    optimizer=Adam(learning_rate=5e-4),  # Start with a slightly reduced learning rate
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Learning Rate Scheduler for warmup
def warmup(epoch):
    if epoch < 5:
        return 5e-5  # Start small during initial epochs
    return 5e-4  # Default learning rate after warmup

lr_scheduler = LearningRateScheduler(warmup)

# Callbacks
model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,  # Reduce learning rate by half when performance plateaus
    patience=3,
    verbose=1,
    min_lr=1e-6
)

# Train the model without EarlyStopping
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=100,  # Ensure full training for 100 epochs
    validation_data=(X_test, y_test),
    callbacks=[model_checkpoint, reduce_lr, lr_scheduler]
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f"Test accuracy: {test_accuracy:.2f}, Test loss: {test_loss:.2f}")

# Plot learning curves
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()